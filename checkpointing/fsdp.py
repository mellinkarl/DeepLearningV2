"""
torchrun --nproc_per_node=[YOUR_NUM_GPUS] fsdp.py
"""

import sys
from io import StringIO

import torch
import torch.distributed as dist
from bignet import BigNet, train_step
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

if __name__ == "__main__":
    dist.init_process_group(backend="nccl")
    torch.cuda.set_device(dist.get_rank())

    print(f"Hello {dist.get_rank()=} {dist.get_world_size()=}")

    if dist.get_rank() != 0:
        sys.stdout = StringIO()

    batch_size, seq_len = 32, 1024
    # orig_model = BigNet(device=f"meta")
    orig_model = BigNet(device=f"cuda:{dist.get_rank()}")
    model = FSDP(orig_model, device_id=dist.get_rank())
    train_step(model, batch_size=batch_size, seq_len=seq_len)
